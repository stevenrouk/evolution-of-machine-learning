{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "NOTEBOOKS_DIR = os.path.abspath(os.getcwd())\n",
    "ROOT_DIR = os.path.split(NOTEBOOKS_DIR)[0]\n",
    "PROCESSED_DATA_DIR = os.path.join(ROOT_DIR, 'data', 'processed')\n",
    "\n",
    "FINAL_DF_FILEPATH = os.path.join(PROCESSED_DATA_DIR, 'final.csv')\n",
    "ML_ONLY_FILEPATH = os.path.join(PROCESSED_DATA_DIR, 'machine_learning_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = pd.read_csv(ML_ONLY_FILEPATH, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_influential_words_per_topic(H, vocabulary):\n",
    "    '''\n",
    "    Print the most influential words of each latent topic.\n",
    "    '''\n",
    "    hand_labels = []\n",
    "    for i, row in enumerate(H):\n",
    "        top_ten = np.argsort(row)[::-1][:10]\n",
    "        print('topic', i)\n",
    "        print('-->', ', '.join(vocabulary[top_ten]))\n",
    "        print(H[i, top_ten])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))\n",
    "tfidf_ml = tfidf_vectorizer.fit_transform(df_ml['description'])\n",
    "features = np.array(tfidf_vectorizer.get_feature_names())\n",
    "nmf_model = NMF(n_components=10, random_state=42)\n",
    "W = nmf_model.fit_transform(tfidf_ml)\n",
    "H = nmf_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1937107,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48564, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1937107)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "--> image, task, domain, model, features, classification, images, tasks, method, language\n",
      "[0.78582865 0.66507851 0.65863228 0.62396726 0.59232523 0.55475786\n",
      " 0.54615639 0.51782957 0.50370434 0.48367955]\n",
      "\n",
      "topic 1\n",
      "--> algorithm, gradient, optimization, convex, stochastic, convergence, algorithms, function, problem, problems\n",
      "[0.68515299 0.65678419 0.62609002 0.58926665 0.55681618 0.53772341\n",
      " 0.4500454  0.42127649 0.41929625 0.38676054]\n",
      "\n",
      "topic 2\n",
      "--> neural, networks, network, neural networks, deep, neural network, deep neural, training, convolutional, layer\n",
      "[1.43846292 1.31056672 1.18750761 0.95406205 0.94484623 0.67429993\n",
      " 0.4853477  0.46451645 0.41949394 0.41464689]\n",
      "\n",
      "topic 3\n",
      "--> policy, reinforcement, reinforcement learning, learning, agent, rl, reward, policies, agents, control\n",
      "[0.98907337 0.79882513 0.78564878 0.78422828 0.58821074 0.52339129\n",
      " 0.40045495 0.38715526 0.37969716 0.37061776]\n",
      "\n",
      "topic 4\n",
      "--> adversarial, attacks, adversarial examples, examples, attack, robustness, perturbations, training, adversarial attacks, adversarial training\n",
      "[1.84934497 0.80645322 0.77138895 0.69117627 0.56573426 0.38599721\n",
      " 0.36098514 0.33527244 0.3169047  0.2943944 ]\n",
      "\n",
      "topic 5\n",
      "--> model, models, inference, latent, bayesian, variational, distribution, variables, gaussian, posterior\n",
      "[0.91535594 0.79111037 0.69626709 0.5745232  0.54726082 0.49011417\n",
      " 0.44941328 0.41183862 0.37126303 0.35129214]\n",
      "\n",
      "topic 6\n",
      "--> graph, graphs, node, nodes, embedding, structure, clustering, network, edges, graph neural\n",
      "[2.40309479 0.83517755 0.48931457 0.40161823 0.33039375 0.25195815\n",
      " 0.22774807 0.21771035 0.20447903 0.19752126]\n",
      "\n",
      "topic 7\n",
      "--> data, learning, machine, machine learning, deep learning, algorithms, classification, deep, training, supervised\n",
      "[1.17899507 1.16405544 0.79552339 0.74222694 0.31367176 0.30986705\n",
      " 0.27724195 0.2528712  0.24533645 0.24216038]\n",
      "\n",
      "topic 8\n",
      "--> time, time series, series, data, forecasting, temporal, series data, prediction, real, lstm\n",
      "[1.36645691 1.27819825 1.27231361 0.29083171 0.27326966 0.26957202\n",
      " 0.25513049 0.22936571 0.19481895 0.19067299]\n",
      "\n",
      "topic 9\n",
      "--> matrix, rank, clustering, low, low rank, sparse, data, dimensional, kernel, algorithm\n",
      "[0.99550055 0.71687372 0.70683472 0.54472562 0.49127331 0.4794298\n",
      " 0.44289816 0.39365262 0.38459659 0.34298818]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_influential_words_per_topic(H, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_tfidf_ml: 19.909547090530396\n",
      "features: 19.91393804550171\n",
      "nfm_model: 19.914159297943115\n",
      "W_ngram: 24.656283140182495\n",
      "H_ngram: 24.65682816505432\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tfidf_vectorizer_2gram_1000 = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1000)\n",
    "tfidf_ml_2gram_1000 = tfidf_vectorizer_2gram_1000.fit_transform(df_ml['description'])\n",
    "print('ngram_tfidf_ml:', time.time() - start)\n",
    "\n",
    "features_2gram_1000 = np.array(tfidf_vectorizer_2gram_1000.get_feature_names())\n",
    "print('features:', time.time() - start)\n",
    "\n",
    "nmf_model_2gram_1000 = NMF(n_components=10, random_state=42)\n",
    "print('nfm_model:', time.time() - start)\n",
    "\n",
    "W_2gram_1000 = nmf_model_2gram_1000.fit_transform(tfidf_ml_2gram_1000)\n",
    "print('W_ngram:', time.time() - start)\n",
    "\n",
    "H_2gram_1000 = nmf_model_2gram_1000.components_\n",
    "print('H_ngram:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "--> features, classification, image, feature, task, method, images, dataset, art, based\n",
      "[1.48551153 1.48294038 1.34095394 1.18026789 1.14577577 1.04819745\n",
      " 0.98955816 0.9442723  0.9205665  0.91896229]\n",
      "\n",
      "topic 1\n",
      "--> neural, networks, network, neural networks, deep, neural network, deep neural, training, convolutional, layer\n",
      "[2.23273679 1.97466684 1.93633016 1.41595231 1.32358887 1.09753677\n",
      " 0.70740265 0.63769829 0.61314055 0.5745794 ]\n",
      "\n",
      "topic 2\n",
      "--> gradient, stochastic, optimization, convergence, convex, descent, gradient descent, stochastic gradient, method, sgd\n",
      "[1.96864527 1.40598325 1.38078347 1.16963396 1.10820961 1.0512812\n",
      " 0.91541112 0.73387071 0.71381813 0.70826078]\n",
      "\n",
      "topic 3\n",
      "--> policy, reinforcement, reinforcement learning, learning, agent, rl, agents, reward, policies, control\n",
      "[1.67296812 1.43572467 1.41010821 1.27256649 1.0849047  0.87955188\n",
      " 0.71826246 0.68722938 0.68086033 0.66274452]\n",
      "\n",
      "topic 4\n",
      "--> model, models, inference, latent, bayesian, variational, variables, distribution, generative, gaussian\n",
      "[2.63340426 1.98617429 1.15368524 0.97287535 0.82460704 0.69769689\n",
      " 0.57310605 0.52838482 0.50900853 0.47723201]\n",
      "\n",
      "topic 5\n",
      "--> graph, graphs, node, nodes, embedding, clustering, structure, network, spectral, embeddings\n",
      "[3.55509751 1.3038235  0.67209713 0.61884327 0.46303018 0.42494464\n",
      " 0.39068165 0.29803321 0.29216324 0.25401581]\n",
      "\n",
      "topic 6\n",
      "--> adversarial, attacks, examples, adversarial examples, training, attack, generative, robustness, gan, perturbations\n",
      "[2.65437934 1.11723651 0.9347033  0.93326812 0.77197548 0.75058596\n",
      " 0.64011088 0.57358126 0.55506529 0.48725175]\n",
      "\n",
      "topic 7\n",
      "--> learning, machine, machine learning, deep learning, deep, algorithms, models, learning algorithms, ml, systems\n",
      "[3.04369598 2.19428778 2.07512024 0.94117955 0.82828482 0.66311036\n",
      " 0.49356154 0.48747867 0.45062432 0.44017249]\n",
      "\n",
      "topic 8\n",
      "--> data, clustering, time, real, series, time series, method, analysis, sets, data sets\n",
      "[3.96592182 1.00344781 0.88360211 0.6759291  0.650615   0.62539939\n",
      " 0.55603879 0.54520568 0.53479465 0.48956328]\n",
      "\n",
      "topic 9\n",
      "--> algorithm, problem, matrix, algorithms, linear, bounds, optimal, sparse, bound, number\n",
      "[1.64305211 1.00380168 0.92868189 0.73008865 0.64502588 0.62848962\n",
      " 0.61721119 0.58189046 0.58006173 0.57247454]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_influential_words_per_topic(H_2gram_1000, features_2gram_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_tfidf_ml: 18.723983764648438\n",
      "features: 18.728272914886475\n",
      "nfm_model: 18.728781938552856\n",
      "W_ngram: 23.996538877487183\n",
      "H_ngram: 23.996723890304565\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tfidf_vectorizer_3gram_1000 = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1000)\n",
    "tfidf_ml_3gram_1000 = tfidf_vectorizer_3gram_1000.fit_transform(df_ml['description'])\n",
    "print('ngram_tfidf_ml:', time.time() - start)\n",
    "\n",
    "features_3gram_1000 = np.array(tfidf_vectorizer_3gram_1000.get_feature_names())\n",
    "print('features:', time.time() - start)\n",
    "\n",
    "nmf_model_3gram_1000 = NMF(n_components=10, random_state=42)\n",
    "print('nfm_model:', time.time() - start)\n",
    "\n",
    "W_3gram_1000 = nmf_model_3gram_1000.fit_transform(tfidf_ml_3gram_1000)\n",
    "print('W_ngram:', time.time() - start)\n",
    "\n",
    "H_3gram_1000 = nmf_model_3gram_1000.components_\n",
    "print('H_ngram:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "--> features, classification, image, feature, task, method, images, dataset, art, based\n",
      "[1.48551153 1.48294038 1.34095394 1.18026789 1.14577577 1.04819745\n",
      " 0.98955816 0.9442723  0.9205665  0.91896229]\n",
      "\n",
      "topic 1\n",
      "--> neural, networks, network, neural networks, deep, neural network, deep neural, training, convolutional, layer\n",
      "[2.23273679 1.97466684 1.93633016 1.41595231 1.32358887 1.09753677\n",
      " 0.70740265 0.63769829 0.61314055 0.5745794 ]\n",
      "\n",
      "topic 2\n",
      "--> gradient, stochastic, optimization, convergence, convex, descent, gradient descent, stochastic gradient, method, sgd\n",
      "[1.96864527 1.40598325 1.38078347 1.16963396 1.10820961 1.0512812\n",
      " 0.91541112 0.73387071 0.71381813 0.70826078]\n",
      "\n",
      "topic 3\n",
      "--> policy, reinforcement, reinforcement learning, learning, agent, rl, agents, reward, policies, control\n",
      "[1.67296812 1.43572467 1.41010821 1.27256649 1.0849047  0.87955188\n",
      " 0.71826246 0.68722938 0.68086033 0.66274452]\n",
      "\n",
      "topic 4\n",
      "--> model, models, inference, latent, bayesian, variational, variables, distribution, generative, gaussian\n",
      "[2.63340426 1.98617429 1.15368524 0.97287535 0.82460704 0.69769689\n",
      " 0.57310605 0.52838482 0.50900853 0.47723201]\n",
      "\n",
      "topic 5\n",
      "--> graph, graphs, node, nodes, embedding, clustering, structure, network, spectral, embeddings\n",
      "[3.55509751 1.3038235  0.67209713 0.61884327 0.46303018 0.42494464\n",
      " 0.39068165 0.29803321 0.29216324 0.25401581]\n",
      "\n",
      "topic 6\n",
      "--> adversarial, attacks, examples, adversarial examples, training, attack, generative, robustness, gan, perturbations\n",
      "[2.65437934 1.11723651 0.9347033  0.93326812 0.77197548 0.75058596\n",
      " 0.64011088 0.57358126 0.55506529 0.48725175]\n",
      "\n",
      "topic 7\n",
      "--> learning, machine, machine learning, deep learning, deep, algorithms, models, learning algorithms, ml, systems\n",
      "[3.04369598 2.19428778 2.07512024 0.94117955 0.82828482 0.66311036\n",
      " 0.49356154 0.48747867 0.45062432 0.44017249]\n",
      "\n",
      "topic 8\n",
      "--> data, clustering, time, real, series, time series, method, analysis, sets, data sets\n",
      "[3.96592182 1.00344781 0.88360211 0.6759291  0.650615   0.62539939\n",
      " 0.55603879 0.54520568 0.53479465 0.48956328]\n",
      "\n",
      "topic 9\n",
      "--> algorithm, problem, matrix, algorithms, linear, bounds, optimal, sparse, bound, number\n",
      "[1.64305211 1.00380168 0.92868189 0.73008865 0.64502588 0.62848962\n",
      " 0.61721119 0.58189046 0.58006173 0.57247454]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_influential_words_per_topic(H_3gram_1000, features_3gram_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we're losing some information by only using 1000 features. For the full model, I'll probably want to remove that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only 2-grams (no single words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_tfidf_ml: 18.301547050476074\n",
      "features: 18.305480003356934\n",
      "nfm_model: 18.30560302734375\n",
      "W_ngram: 19.193109035491943\n",
      "H_ngram: 19.19334626197815\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tfidf_vectorizer_2_only_gram_1000 = TfidfVectorizer(stop_words='english', ngram_range=(2, 2), max_features=1000)\n",
    "tfidf_ml_2_only_gram_1000 = tfidf_vectorizer_2_only_gram_1000.fit_transform(df_ml['description'])\n",
    "print('ngram_tfidf_ml:', time.time() - start)\n",
    "\n",
    "features_2_only_gram_1000 = np.array(tfidf_vectorizer_2_only_gram_1000.get_feature_names())\n",
    "print('features:', time.time() - start)\n",
    "\n",
    "nmf_model_2_only_gram_1000 = NMF(n_components=10, random_state=42)\n",
    "print('nfm_model:', time.time() - start)\n",
    "\n",
    "W_2_only_gram_1000 = nmf_model_2_only_gram_1000.fit_transform(tfidf_ml_2_only_gram_1000)\n",
    "print('W_ngram:', time.time() - start)\n",
    "\n",
    "H_2_only_gram_1000 = nmf_model_2_only_gram_1000.components_\n",
    "print('H_ngram:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "--> neural networks, deep neural, convolutional neural, recurrent neural, networks dnns, adversarial examples, networks cnns, artificial neural, cifar 10, training deep\n",
      "[6.12176074 2.42153113 0.93374954 0.61003035 0.56534016 0.49000115\n",
      " 0.4570256  0.30042007 0.28475252 0.28236073]\n",
      "\n",
      "topic 1\n",
      "--> machine learning, learning algorithms, learning models, learning techniques, learning ml, learning methods, learning model, learning systems, learning algorithm, using machine\n",
      "[4.15484352 0.60839516 0.51489602 0.3769146  0.34104413 0.28882068\n",
      " 0.16535568 0.16245515 0.15866517 0.15314164]\n",
      "\n",
      "topic 2\n",
      "--> state art, paper propose, art methods, propose novel, experimental results, proposed method, art performance, art results, outperforms state, semi supervised\n",
      "[3.81160578 0.91408228 0.77320177 0.75042212 0.6823414  0.66813624\n",
      " 0.66683694 0.57154162 0.54240953 0.53624966]\n",
      "\n",
      "topic 3\n",
      "--> deep learning, learning models, learning based, learning model, learning methods, learning techniques, using deep, based deep, learning approach, computer vision\n",
      "[4.14411417 0.57460804 0.40137858 0.24508282 0.229364   0.2078409\n",
      " 0.19207024 0.17074426 0.15204876 0.14139488]\n",
      "\n",
      "topic 4\n",
      "--> reinforcement learning, deep reinforcement, learning rl, learning algorithms, learning algorithm, multi agent, model free, markov decision, policy gradient, value function\n",
      "[3.89822619 1.11266011 0.71680915 0.4544543  0.38122842 0.3570464\n",
      " 0.34962813 0.33343175 0.3270009  0.31950965]\n",
      "\n",
      "topic 5\n",
      "--> neural network, convolutional neural, deep neural, recurrent neural, network cnn, network architecture, network models, network based, network architectures, network model\n",
      "[3.74698608 0.77465742 0.51140896 0.45443608 0.3903508  0.3716029\n",
      " 0.31417608 0.30461537 0.26048806 0.24813871]\n",
      "\n",
      "topic 6\n",
      "--> time series, series data, short term, recurrent neural, long short, term memory, anomaly detection, memory lstm, long term, gaussian process\n",
      "[3.8701842  0.7456514  0.24120618 0.2187554  0.19222176 0.19100886\n",
      " 0.18596352 0.11878053 0.10760654 0.10451894]\n",
      "\n",
      "topic 7\n",
      "--> gradient descent, stochastic gradient, et al, non convex, convergence rate, low rank, convex optimization, optimization problems, large scale, descent sgd\n",
      "[2.13384906 1.44085582 1.3548892  0.74341844 0.63337146 0.62520183\n",
      " 0.57975219 0.46676381 0.43857727 0.4290814 ]\n",
      "\n",
      "topic 8\n",
      "--> real world, world datasets, world data, synthetic real, world applications, data sets, paper propose, experiments real, propose novel, proposed method\n",
      "[2.97889857 0.67250519 0.67117149 0.53834169 0.44656725 0.38321518\n",
      " 0.25953892 0.24812589 0.24781013 0.22033638]\n",
      "\n",
      "topic 9\n",
      "--> high dimensional, dimensional data, low dimensional, data sets, feature selection, dimensionality reduction, real data, data analysis, proposed method, data points\n",
      "[3.09006925 0.88574159 0.73186342 0.50015083 0.41174026 0.31949434\n",
      " 0.31934246 0.27792357 0.27024728 0.26686617]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_influential_words_per_topic(H_2_only_gram_1000, features_2_only_gram_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only 3-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngram_tfidf_ml: 28.08918309211731\n",
      "features: 28.097098112106323\n",
      "nfm_model: 28.097248077392578\n",
      "W_ngram: 28.749913215637207\n",
      "H_ngram: 28.752339124679565\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "tfidf_vectorizer_3_only_gram_1000 = TfidfVectorizer(stop_words='english', ngram_range=(3, 3), max_features=1000)\n",
    "tfidf_ml_3_only_gram_1000 = tfidf_vectorizer_3_only_gram_1000.fit_transform(df_ml['description'])\n",
    "print('ngram_tfidf_ml:', time.time() - start)\n",
    "\n",
    "features_3_only_gram_1000 = np.array(tfidf_vectorizer_3_only_gram_1000.get_feature_names())\n",
    "print('features:', time.time() - start)\n",
    "\n",
    "nmf_model_3_only_gram_1000 = NMF(n_components=10, random_state=42)\n",
    "print('nfm_model:', time.time() - start)\n",
    "\n",
    "W_3_only_gram_1000 = nmf_model_3_only_gram_1000.fit_transform(tfidf_ml_3_only_gram_1000)\n",
    "print('W_ngram:', time.time() - start)\n",
    "\n",
    "H_3_only_gram_1000 = nmf_model_3_only_gram_1000.components_\n",
    "print('H_ngram:', time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "--> deep neural networks, neural networks dnns, training deep neural, neural networks dnn, using deep neural, state art performance, learning deep neural, based deep neural, mnist cifar 10, neural networks trained\n",
      "[4.84470172 1.18548741 0.39347682 0.25046856 0.16658612 0.11803193\n",
      " 0.11452266 0.09864378 0.09585715 0.09381212]\n",
      "\n",
      "topic 1\n",
      "--> convolutional neural networks, neural networks cnns, deep convolutional neural, neural networks cnn, state art performance, state art results, based convolutional neural, graph convolutional neural, achieve state art, using convolutional neural\n",
      "[4.21062478 1.72590409 0.70024894 0.45661084 0.22809815 0.14496153\n",
      " 0.1151631  0.10922772 0.10885449 0.09574735]\n",
      "\n",
      "topic 2\n",
      "--> convolutional neural network, neural network cnn, deep convolutional neural, neural network architecture, neural network trained, based convolutional neural, recurrent neural network, deep learning based, using convolutional neural, neural network model\n",
      "[4.41966986 2.14260944 0.96525948 0.22506674 0.18213254 0.17842653\n",
      " 0.15832561 0.15559781 0.14636349 0.13972815]\n",
      "\n",
      "topic 3\n",
      "--> deep neural network, neural network dnn, neural network models, neural network based, neural network architectures, neural network architecture, neural network model, using deep neural, train deep neural, state art performance\n",
      "[3.57500809 0.87467441 0.26612885 0.23854522 0.18084063 0.16682341\n",
      " 0.15864783 0.13107544 0.11360991 0.10354318]\n",
      "\n",
      "topic 4\n",
      "--> generative adversarial networks, adversarial networks gans, adversarial networks gan, generative adversarial network, conditional generative adversarial, deep generative models, semi supervised learning, based generative adversarial, state art results, using generative adversarial\n",
      "[3.52805255 2.17153271 0.28467641 0.17905366 0.17590631 0.16213417\n",
      " 0.15103219 0.12427985 0.12024686 0.1167497 ]\n",
      "\n",
      "topic 5\n",
      "--> stochastic gradient descent, gradient descent sgd, gradient descent algorithm, using stochastic gradient, machine learning ml, training deep neural, gradient descent method, non convex optimization, machine learning problems, deep learning models\n",
      "[4.54795512 1.83588398 0.28030298 0.20855981 0.16188353 0.16009637\n",
      " 0.14762563 0.12650181 0.11922231 0.11892729]\n",
      "\n",
      "topic 6\n",
      "--> deep reinforcement learning, reinforcement learning rl, reinforcement learning drl, reinforcement learning algorithms, learning rl algorithms, markov decision process, based reinforcement learning, using deep reinforcement, reinforcement learning algorithm, state art performance\n",
      "[2.52493307 2.37196401 0.36595989 0.3137493  0.16995776 0.1668842\n",
      " 0.14298081 0.14035116 0.13255881 0.13049569]\n",
      "\n",
      "topic 7\n",
      "--> recurrent neural networks, short term memory, long short term, neural networks rnns, term memory lstm, recurrent neural network, neural network rnn, natural language processing, neural networks rnn, time series data\n",
      "[2.20515867 1.33171075 1.3265426  0.85685233 0.76701153 0.71579001\n",
      " 0.26669968 0.25153259 0.23620593 0.17471045]\n",
      "\n",
      "topic 8\n",
      "--> machine learning models, machine learning algorithms, machine learning techniques, machine learning model, support vector machine, use machine learning, deep learning models, support vector machines, machine learning ml, machine learning methods\n",
      "[2.97535995 1.729359   0.31367773 0.24385502 0.24142002 0.16942524\n",
      " 0.16015623 0.15968846 0.15799916 0.15542783]\n",
      "\n",
      "topic 9\n",
      "--> state art methods, real world datasets, real world data, synthetic real world, high dimensional data, outperforms state art, https github com, compared state art, paper propose novel, available https github\n",
      "[1.83658508 1.59783698 1.08962213 0.93294547 0.88555094 0.77334194\n",
      " 0.55173256 0.51281059 0.44395262 0.33249804]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_influential_words_per_topic(H_3_only_gram_1000, features_3_only_gram_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
