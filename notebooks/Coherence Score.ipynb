{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "NOTEBOOKS_DIR = os.path.abspath(os.getcwd())\n",
    "ROOT_DIRECTORY = os.path.split(NOTEBOOKS_DIR)[0]\n",
    "DATA_DIRECTORY_RAW = os.path.join(ROOT_DIRECTORY, 'data', 'raw')\n",
    "DATA_DIRECTORY_PROCESSED = os.path.join(ROOT_DIRECTORY, 'data', 'processed')\n",
    "DATA_DIRECTORY_PROCESSED_DFS = os.path.join(ROOT_DIRECTORY, 'data', 'processed', 'dfs')\n",
    "MODELS_DIRECTORY = os.path.join(ROOT_DIRECTORY, 'models')\n",
    "\n",
    "FINAL_DF_FILEPATH = os.path.join(DATA_DIRECTORY_PROCESSED, 'final.csv')\n",
    "ML_ONLY_FILEPATH = os.path.join(DATA_DIRECTORY_PROCESSED, 'machine_learning_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(ML_ONLY_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "loading vectorizer\n",
      "loading weights\n"
     ]
    }
   ],
   "source": [
    "n_components = 10\n",
    "model_filename = os.path.join(MODELS_DIRECTORY, f'nmf_{n_components}_model.pkl')\n",
    "vectorizer_filename = os.path.join(MODELS_DIRECTORY, f'vectorizer_tfidf.pkl')\n",
    "weights_filename = os.path.join(MODELS_DIRECTORY, f'nmf_{n_components}_weights_W.pkl')\n",
    "\n",
    "print('loading model')\n",
    "with open(model_filename, 'rb') as f:\n",
    "    nmf_model = pickle.load(f)\n",
    "\n",
    "print('loading vectorizer')\n",
    "with open(vectorizer_filename, 'rb') as f:\n",
    "    tfidf_vectorizer = pickle.load(f)\n",
    "\n",
    "print('loading weights')\n",
    "with open(weights_filename, 'rb') as f:\n",
    "    W = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing using gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_corpus, common_dictionary, common_texts\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "model = LdaModel(common_corpus, 5, common_dictionary)\n",
    "\n",
    "cm = CoherenceModel(model=model, corpus=common_corpus, coherence='u_mass')\n",
    "coherence = cm.get_coherence()  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-14.678142504651342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.ldamodel.LdaModel at 0x102f36c10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.150*\"user\" + 0.103*\"system\" + 0.103*\"computer\" + 0.103*\"response\" + 0.103*\"interface\" + 0.103*\"time\" + 0.057*\"trees\" + 0.056*\"graph\" + 0.056*\"survey\" + 0.056*\"minors\"'),\n",
       " (1,\n",
       "  '0.346*\"trees\" + 0.060*\"graph\" + 0.060*\"minors\" + 0.060*\"interface\" + 0.060*\"time\" + 0.059*\"human\" + 0.059*\"system\" + 0.059*\"survey\" + 0.059*\"eps\" + 0.059*\"user\"'),\n",
       " (2,\n",
       "  '0.341*\"system\" + 0.186*\"eps\" + 0.186*\"human\" + 0.033*\"trees\" + 0.032*\"graph\" + 0.032*\"survey\" + 0.032*\"minors\" + 0.032*\"interface\" + 0.032*\"user\" + 0.032*\"time\"'),\n",
       " (3,\n",
       "  '0.296*\"graph\" + 0.162*\"survey\" + 0.161*\"minors\" + 0.161*\"trees\" + 0.028*\"human\" + 0.028*\"system\" + 0.028*\"user\" + 0.028*\"time\" + 0.028*\"response\" + 0.028*\"eps\"'),\n",
       " (4,\n",
       "  '0.087*\"trees\" + 0.084*\"graph\" + 0.083*\"system\" + 0.083*\"minors\" + 0.083*\"time\" + 0.083*\"eps\" + 0.083*\"human\" + 0.083*\"user\" + 0.083*\"response\" + 0.083*\"interface\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'computer'),\n",
       " (1, 'human'),\n",
       " (2, 'interface'),\n",
       " (3, 'response'),\n",
       " (4, 'survey'),\n",
       " (5, 'system'),\n",
       " (6, 'time'),\n",
       " (7, 'user'),\n",
       " (8, 'eps'),\n",
       " (9, 'trees'),\n",
       " (10, 'graph'),\n",
       " (11, 'minors')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(common_dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advantage',\n",
       " 'algorithms',\n",
       " 'algorithms',\n",
       " 'all',\n",
       " 'alternative',\n",
       " 'an',\n",
       " 'analysis',\n",
       " 'analysis',\n",
       " 'analyzed',\n",
       " 'analyzed']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(simple_preprocess(df.iloc[0]['description']))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = df['description'].map(simple_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [this, paper, presents, unified, framework, to...\n",
       "1        [multitask, learning, can, be, effective, when...\n",
       "2        [we, investigate, generic, problem, of, learni...\n",
       "3        [novel, unified, bayesian, framework, for, net...\n",
       "4        [this, work, considers, an, estimation, task, ...\n",
       "                               ...                        \n",
       "48559    [maximum, posteriori, map, inference, over, di...\n",
       "48560    [this, paper, is, survey, of, dictionary, scre...\n",
       "48561    [the, problem, of, secure, friend, discovery, ...\n",
       "48562    [two, complementary, approaches, have, been, e...\n",
       "48563    [this, monograph, presents, the, main, complex...\n",
       "Name: description, Length: 48564, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dictionary and corpus, then train NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "ml_dictionary = Dictionary(processed_text)\n",
    "ml_corpus = [ml_dictionary.doc2bow(text) for text in processed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the corpus.\n",
    "nmf = Nmf(ml_corpus, 10, ml_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nmf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (1, 2),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 2),\n",
       " (6, 2),\n",
       " (7, 8),\n",
       " (8, 2),\n",
       " (9, 2),\n",
       " (10, 1),\n",
       " (11, 1),\n",
       " (12, 1),\n",
       " (13, 1),\n",
       " (14, 1),\n",
       " (15, 2),\n",
       " (16, 1),\n",
       " (17, 1),\n",
       " (18, 1),\n",
       " (19, 1),\n",
       " (20, 1),\n",
       " (21, 4),\n",
       " (22, 1),\n",
       " (23, 2),\n",
       " (24, 1),\n",
       " (25, 1),\n",
       " (26, 1),\n",
       " (27, 1),\n",
       " (28, 1),\n",
       " (29, 1),\n",
       " (30, 1),\n",
       " (31, 1),\n",
       " (32, 2),\n",
       " (33, 2),\n",
       " (34, 1),\n",
       " (35, 4),\n",
       " (36, 1),\n",
       " (37, 1),\n",
       " (38, 2),\n",
       " (39, 1),\n",
       " (40, 4),\n",
       " (41, 1),\n",
       " (42, 1),\n",
       " (43, 1),\n",
       " (44, 2),\n",
       " (45, 1),\n",
       " (46, 4),\n",
       " (47, 3),\n",
       " (48, 2),\n",
       " (49, 1),\n",
       " (50, 1),\n",
       " (51, 1),\n",
       " (52, 1),\n",
       " (53, 1),\n",
       " (54, 2),\n",
       " (55, 1),\n",
       " (56, 1),\n",
       " (57, 1),\n",
       " (58, 2),\n",
       " (59, 1),\n",
       " (60, 10),\n",
       " (61, 1),\n",
       " (62, 1),\n",
       " (63, 5),\n",
       " (64, 1),\n",
       " (65, 1),\n",
       " (66, 2),\n",
       " (67, 1),\n",
       " (68, 1),\n",
       " (69, 1),\n",
       " (70, 6),\n",
       " (71, 1),\n",
       " (72, 1),\n",
       " (73, 1),\n",
       " (74, 2),\n",
       " (75, 1),\n",
       " (76, 1),\n",
       " (77, 3),\n",
       " (78, 9),\n",
       " (79, 1),\n",
       " (80, 4),\n",
       " (81, 1),\n",
       " (82, 1),\n",
       " (83, 1),\n",
       " (84, 1),\n",
       " (85, 1),\n",
       " (86, 12),\n",
       " (87, 2),\n",
       " (88, 1),\n",
       " (89, 1),\n",
       " (90, 1),\n",
       " (91, 1),\n",
       " (92, 1),\n",
       " (93, 1),\n",
       " (94, 1),\n",
       " (95, 3),\n",
       " (96, 2),\n",
       " (97, 1),\n",
       " (98, 1),\n",
       " (99, 2),\n",
       " (100, 1),\n",
       " (101, 1),\n",
       " (102, 1),\n",
       " (103, 2),\n",
       " (104, 1),\n",
       " (105, 1),\n",
       " (106, 2),\n",
       " (107, 12),\n",
       " (108, 1),\n",
       " (109, 2),\n",
       " (110, 1),\n",
       " (111, 1),\n",
       " (112, 1),\n",
       " (113, 1),\n",
       " (114, 1),\n",
       " (115, 1),\n",
       " (116, 2),\n",
       " (117, 1),\n",
       " (118, 1),\n",
       " (119, 6),\n",
       " (120, 2),\n",
       " (121, 2),\n",
       " (122, 1),\n",
       " (123, 1),\n",
       " (124, 1),\n",
       " (125, 19),\n",
       " (126, 1),\n",
       " (127, 2),\n",
       " (128, 2),\n",
       " (129, 1),\n",
       " (130, 3),\n",
       " (131, 6),\n",
       " (132, 1),\n",
       " (133, 1),\n",
       " (134, 1),\n",
       " (135, 1),\n",
       " (136, 1),\n",
       " (137, 2),\n",
       " (138, 2),\n",
       " (139, 1),\n",
       " (140, 3),\n",
       " (141, 1),\n",
       " (142, 1),\n",
       " (143, 3),\n",
       " (144, 1),\n",
       " (145, 1),\n",
       " (146, 1),\n",
       " (147, 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import preprocess_documents, preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper',\n",
       " 'present',\n",
       " 'unifi',\n",
       " 'framework',\n",
       " 'tackl',\n",
       " 'estim',\n",
       " 'problem',\n",
       " 'digit',\n",
       " 'signal',\n",
       " 'process']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_documents([df.iloc[0]['description']])[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This paper presents unified framework tackle estimation problems Digital Signal Processing (DSP) Support Vector Machines (SVMs). The use SVMs estimation problems traditionally limited mere use black-box model. Noting limitations literature, advantage properties Mercer's kernels functional analysis develop family SVM methods estimation DSP. Three types signal model equations analyzed. First, specific time-signal structure assumed model underlying generated data, linear signal model (so called Primal Signal Model formulation) stated analyzed. Then, non-linear versions signal structure readily developed following different approaches. On hand, signal model equation written reproducing kernel Hilbert spaces (RKHS) well-known RKHS Signal Model formulation, Mercer's kernels readily SVM non-linear algorithms. On hand, alternative common Dual Signal Model formulation, signal expansion auxiliary signal model equation given non-linear regression time instant observed time series. These building blocks generate different novel SVM-based methods problems signal estimation, deal important ones DSP. We illustrate usefulness methodology defining SVM algorithms linear non-linear identification, spectral analysis, nonuniform interpolation, sparse deconvolution, array processing. The performance developed SVM methods compared standard approaches settings. The experimental results illustrate generality, simplicity, capabilities proposed SVM framework DSP.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(df.iloc[0]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the. big'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(\"A a The the. The big\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  This paper presents a unified framework to tackle estimation problems in\\nDigital Signal Processing (DSP) using Support Vector Machines (SVMs). The use\\nof SVMs in estimation problems has been traditionally limited to its mere use\\nas a black-box model. Noting such limitations in the literature, we take\\nadvantage of several properties of Mercer's kernels and functional analysis to\\ndevelop a family of SVM methods for estimation in DSP. Three types of signal\\nmodel equations are analyzed. First, when a specific time-signal structure is\\nassumed to model the underlying system that generated the data, the linear\\nsignal model (so called Primal Signal Model formulation) is first stated and\\nanalyzed. Then, non-linear versions of the signal structure can be readily\\ndeveloped by following two different approaches. On the one hand, the signal\\nmodel equation is written in reproducing kernel Hilbert spaces (RKHS) using the\\nwell-known RKHS Signal Model formulation, and Mercer's kernels are readily used\\nin SVM non-linear algorithms. On the other hand, in the alternative and not so\\ncommon Dual Signal Model formulation, a signal expansion is made by using an\\nauxiliary signal model equation given by a non-linear regression of each time\\ninstant in the observed time series. These building blocks can be used to\\ngenerate different novel SVM-based methods for problems of signal estimation,\\nand we deal with several of the most important ones in DSP. We illustrate the\\nusefulness of this methodology by defining SVM algorithms for linear and\\nnon-linear system identification, spectral analysis, nonuniform interpolation,\\nsparse deconvolution, and array processing. The performance of the developed\\nSVM methods is compared to standard approaches in all these settings. The\\nexperimental results illustrate the generality, simplicity, and capabilities of\\nthe proposed SVM framework for DSP.\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = preprocess_documents(df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paper',\n",
       " 'present',\n",
       " 'unifi',\n",
       " 'framework',\n",
       " 'tackl',\n",
       " 'estim',\n",
       " 'problem',\n",
       " 'digit',\n",
       " 'signal',\n",
       " 'process']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts\n",
    "ml_dictionary = Dictionary(processed_text)\n",
    "ml_corpus = [ml_dictionary.doc2bow(text) for text in processed_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at different number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "['algorithm', 'deep', 'learn', 'model', 'network', 'neural', 'perform', 'propos', 'task', 'train']\n",
      "\n",
      "topic 1\n",
      "['data', 'featur', 'gener', 'imag', 'learn', 'method', 'model', 'network', 'propos', 'train']\n",
      "\n",
      "topic 2\n",
      "['algorithm', 'data', 'distribut', 'estim', 'function', 'method', 'model', 'optim', 'problem', 'propos']\n",
      "\n",
      "topic 3\n",
      "['approach', 'base', 'data', 'learn', 'machin', 'model', 'predict', 'propos', 'time', 'user']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LdaModel(ml_corpus, 4, ml_dictionary)\n",
    "for i in range(4):\n",
    "    print(f\"topic {i}\")\n",
    "    print(sorted([x[0] for x in model.show_topic(i)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(ml_corpus, 10, ml_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoherenceModel(model=model, corpus=ml_corpus, coherence='u_mass')\n",
    "coherence = cm.get_coherence()  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.680781506020875"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "['adversari', 'data', 'gener', 'imag', 'kernel', 'learn', 'model', 'network', 'propos', 'train']\n",
      "\n",
      "topic 1\n",
      "['approach', 'base', 'classif', 'data', 'dataset', 'featur', 'imag', 'method', 'model', 'propos']\n",
      "\n",
      "topic 2\n",
      "['approach', 'bayesian', 'data', 'infer', 'learn', 'method', 'model', 'predict', 'propos', 'variabl']\n",
      "\n",
      "topic 3\n",
      "['approach', 'data', 'domain', 'learn', 'method', 'model', 'perform', 'propos', 'task', 'train']\n",
      "\n",
      "topic 4\n",
      "['algorithm', 'cluster', 'data', 'distribut', 'estim', 'measur', 'probabl', 'problem', 'sampl', 'set']\n",
      "\n",
      "topic 5\n",
      "['algorithm', 'data', 'dimension', 'function', 'graph', 'matrix', 'method', 'optim', 'problem', 'propos']\n",
      "\n",
      "topic 6\n",
      "['algorithm', 'bound', 'converg', 'function', 'gradient', 'learn', 'method', 'optim', 'problem', 'stochast']\n",
      "\n",
      "topic 7\n",
      "['attack', 'commun', 'data', 'detect', 'learn', 'model', 'network', 'node', 'system', 'time']\n",
      "\n",
      "topic 8\n",
      "['agent', 'algorithm', 'learn', 'onlin', 'optim', 'problem', 'recommend', 'regret', 'set', 'user']\n",
      "\n",
      "topic 9\n",
      "['architectur', 'convolut', 'deep', 'layer', 'learn', 'model', 'network', 'neural', 'perform', 'train']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"topic {i}\")\n",
    "    print(sorted([x[0] for x in model.show_topic(i)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "-1.382871453529041\n",
      "4\n",
      "-1.331313371232071\n",
      "5\n",
      "-1.3563345571951215\n",
      "8\n",
      "-1.5502184691140024\n",
      "10\n",
      "-1.602341419768642\n",
      "12\n",
      "-1.6915873802794137\n",
      "15\n",
      "-1.7205349996045516\n",
      "20\n",
      "-2.1730298366466374\n"
     ]
    }
   ],
   "source": [
    "for i in (3, 4, 5, 8, 10, 12, 15, 20):\n",
    "    print(i)\n",
    "    m = LdaModel(ml_corpus, i, ml_dictionary)\n",
    "    cm = CoherenceModel(model=m, corpus=ml_corpus, coherence='u_mass')\n",
    "    print(cm.get_coherence())  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0\n",
      "['algorithm', 'converg', 'convex', 'function', 'gradient', 'method', 'optim', 'problem', 'propos', 'stochast']\n",
      "\n",
      "topic 1\n",
      "['applic', 'base', 'data', 'deep', 'learn', 'machin', 'network', 'research', 'system', 'time']\n",
      "\n",
      "topic 2\n",
      "['action', 'agent', 'control', 'environ', 'learn', 'polici', 'reinforc', 'robot', 'state', 'task']\n",
      "\n",
      "topic 3\n",
      "['activ', 'convolut', 'deep', 'function', 'input', 'layer', 'network', 'neural', 'train', 'weight']\n",
      "\n",
      "topic 4\n",
      "['data', 'dimension', 'distanc', 'kernel', 'low', 'matrix', 'method', 'propos', 'space', 'vector']\n",
      "\n",
      "topic 5\n",
      "['algorithm', 'dictionari', 'learn', 'mathcal', 'matrix', 'problem', 'quantum', 'signal', 'spars', 'time']\n",
      "\n",
      "topic 6\n",
      "['data', 'label', 'learn', 'method', 'model', 'network', 'perform', 'propos', 'task', 'train']\n",
      "\n",
      "topic 7\n",
      "['base', 'chang', 'data', 'detect', 'filter', 'model', 'predict', 'price', 'propos', 'time']\n",
      "\n",
      "topic 8\n",
      "['base', 'factor', 'item', 'prefer', 'propos', 'rank', 'rate', 'recommend', 'sourc', 'user']\n",
      "\n",
      "topic 9\n",
      "['bound', 'class', 'distribut', 'error', 'function', 'gener', 'learn', 'loss', 'result', 'set']\n",
      "\n",
      "topic 10\n",
      "['algorithm', 'comput', 'data', 'distribut', 'effici', 'estim', 'method', 'perform', 'propos', 'sampl']\n",
      "\n",
      "topic 11\n",
      "['algorithm', 'arm', 'bandit', 'cluster', 'decis', 'onlin', 'optim', 'problem', 'regret', 'set']\n",
      "\n",
      "topic 12\n",
      "['data', 'inform', 'interpret', 'learn', 'machin', 'model', 'patient', 'predict', 'provid', 'studi']\n",
      "\n",
      "topic 13\n",
      "['bias', 'cost', 'electr', 'energi', 'forecast', 'imbal', 'persist', 'power', 'sensit', 'tabl']\n",
      "\n",
      "topic 14\n",
      "['commun', 'edg', 'embed', 'graph', 'link', 'method', 'model', 'network', 'node', 'structur']\n",
      "\n",
      "topic 15\n",
      "['base', 'classif', 'classifi', 'dataset', 'detect', 'featur', 'imag', 'method', 'object', 'propos']\n",
      "\n",
      "topic 16\n",
      "['base', 'gener', 'languag', 'model', 'network', 'propos', 'sequenc', 'task', 'train', 'word']\n",
      "\n",
      "topic 17\n",
      "['adversari', 'attack', 'exampl', 'learn', 'model', 'perturb', 'propos', 'robust', 'search', 'train']\n",
      "\n",
      "topic 18\n",
      "['data', 'distribut', 'featur', 'gener', 'latent', 'method', 'model', 'propos', 'select', 'variabl']\n",
      "\n",
      "topic 19\n",
      "['bayesian', 'data', 'estim', 'gaussian', 'infer', 'method', 'model', 'paramet', 'process', 'regress']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(f\"topic {i}\")\n",
    "    print(sorted([x[0] for x in m.show_topic(i)]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- [(19, 0.42074344), (4, 0.32120946), (1, 0.16010426), (9, 0.065056354), (17, 0.019072272)]\n",
      "\n",
      "1 -- [(18, 0.25287732), (6, 0.21733043), (15, 0.19612978), (0, 0.14315172), (9, 0.13410266), (3, 0.04862747)]\n",
      "\n",
      "2 -- [(19, 0.47108638), (14, 0.14372198), (4, 0.14245492), (10, 0.13533966), (9, 0.09765017)]\n",
      "\n",
      "3 -- [(14, 0.6709431), (19, 0.13877225), (0, 0.09675104), (17, 0.083773136)]\n",
      "\n",
      "4 -- [(10, 0.59172595), (18, 0.20567958), (6, 0.1410145), (2, 0.05182047)]\n",
      "\n",
      "5 -- [(18, 0.30910546), (4, 0.24588783), (6, 0.12937838), (19, 0.09191819), (0, 0.086828396), (12, 0.080965504), (3, 0.047124192)]\n",
      "\n",
      "6 -- [(0, 0.63222873), (5, 0.13759778), (10, 0.124387175), (19, 0.052485265), (6, 0.043021303)]\n",
      "\n",
      "7 -- [(0, 0.6585103), (5, 0.24158467), (8, 0.09140255)]\n",
      "\n",
      "8 -- [(0, 0.5277565), (6, 0.30365962), (9, 0.11588411), (16, 0.026188208), (4, 0.019257301)]\n",
      "\n",
      "9 -- [(14, 0.47788218), (9, 0.15134759), (19, 0.14869864), (17, 0.12767899), (15, 0.06180227), (4, 0.024634993)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(i, \"--\", sorted(m.get_document_topics(ml_corpus[i]), key=lambda x: x[1], reverse=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bayesian Discovery of Threat Networks'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A novel unified Bayesian framework for network detection is developed, under\n",
      "which a detection algorithm is derived based on random walks on graphs. The\n",
      "algorithm detects threat networks using partial observations of their activity,\n",
      "and is proved to be optimum in the Neyman-Pearson sense. The algorithm is\n",
      "defined by a graph, at least one observation, and a diffusion model for threat.\n",
      "A link to well-known spectral detection methods is provided, and the\n",
      "equivalence of the random walk and harmonic solutions to the Bayesian\n",
      "formulation is proven. A general diffusion model is introduced that utilizes\n",
      "spatio-temporal relationships between vertices, and is used for a specific\n",
      "space-time formulation that leads to significant performance improvements on\n",
      "coordinated covert networks. This performance is demonstrated using a new\n",
      "hybrid mixed-membership blockmodel introduced to simulate random covert\n",
      "networks with realistic properties.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[3]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Non-Linear Feature Maps\n",
      "  Feature selection plays a pivotal role in learning, particularly in areas\n",
      "were parsimonious features can provide insight into the underlying process,\n",
      "such as biology. Recent approaches for non-linear feature selection employing\n",
      "greedy optimisation of Centred Kernel Target Alignment(KTA), while exhibiting\n",
      "strong results in terms of generalisation accuracy and sparsity, can become\n",
      "computationally prohibitive for high-dimensional datasets. We propose randSel,\n",
      "a randomised feature selection algorithm, with attractive scaling properties.\n",
      "Our theoretical analysis of randSel provides strong probabilistic guarantees\n",
      "for the correct identification of relevant features. Experimental results on\n",
      "real and artificial data, show that the method successfully identifies\n",
      "effective features, performing better than a number of competitive approaches.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[5]['title'])\n",
    "print(df.iloc[5]['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
